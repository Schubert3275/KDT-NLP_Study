{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 한글 데이터셋  RNN ] <hr>\n",
    "- 데이터셋 : Kopora의 NAVER Sentiment Movie Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] 데이터 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ===> 모듈 로딩\n",
    "from Korpora import Korpora \n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ===> 데이터 로딩\n",
    "corpous = Korpora.load('nsmc')\n",
    "print(corpous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmcDF = pd.DataFrame(corpous.test)\n",
    "nsmcDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsmIter=nsmcDF.itertuples()\n",
    "for item in nsmIter:\n",
    "    print(item, item.text, item.label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 데이터셋 준비 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, corpus):\n",
    "        nsmcDF = pd.DataFrame(corpus).fillna('')\n",
    "\n",
    "        x_data = nsmcDF['text'].values\n",
    "        self.x_data = x_data\n",
    "        self.y_data = nsmcDF['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, vocab, tokenizer):\n",
    "#         super().__init__()\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.vocab = vocab\n",
    "#         self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts.iloc[idx]\n",
    "#         label = self.labels.iloc[idx]\n",
    "#         return self.vocab(self.tokenizer(text)), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = CustomDataset(corpous.train)\n",
    "testDS = CustomDataset(corpous.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'trainDS =>  {len(trainDS)}개   testDS =>  {len(testDS)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in trainDS:\n",
    "    print(label, text)\n",
    "    break\n",
    "\n",
    "for label, text in testDS:\n",
    "    print(label, text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 단어 사전 생성 <hr>\n",
    "    * 토큰화 진행 ==> 형태소 분석기 선택 \n",
    "    * 단어 사전 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-1] 토큰화 관련 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈 로딩\n",
    "from konlpy.tag import Okt\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰관련 특별 문자\n",
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰화 인스턴스 생성\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰 제너레이터 함수 : 데이터 추출하여 토큰화 \n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        # 라벨, 텍스트 --> 텍스트 토큰화\n",
    "        yield tokenizer.morphs(text, stem=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-2] 토큰화 ===> 단어/어휘 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 토큰화 및 단어/어휘 사전 생성\n",
    "VOCAB = build_vocab_from_iterator(\n",
    "    yield_tokens(trainDS),\n",
    "    min_freq=2,\n",
    "    specials= [PAD, UNK],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "### <UNK> 인덱스 설정\n",
    "VOCAB.set_default_index(VOCAB[UNK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VOCAB 메서드 \n",
    "VOCAB.get_itos()[:9], VOCAB.get_stoi()['영화'], VOCAB.get_stoi()[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 텍스트 >>>> 정수 인코딩\n",
    "text_pipeline = lambda x: VOCAB(tokenizer.morphs(text, stem=True))\n",
    "\n",
    "### ===> 레이틀 >>> 정수 인코딩 (0~3)\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2-3] 인코딩 & 디코딩 인덱싱 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인코딩 : 문자 >>>> 숫자로 변환\n",
    "token_to_id ={ label : id  for label, id in VOCAB.get_stoi().items()}\n",
    "\n",
    "### 디코딩 : 숫자 >>>> 문자로 변환\n",
    "id_to_token ={ id : label  for label, id in VOCAB.get_stoi().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [3] 데이터 로더 생성 <hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 모듈로딩\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 실행 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> DataLoader에서 배치크기만큼 데이터셋 반환 함수 \n",
    "def collate_batch(batch):\n",
    "    # 배치크기 만큼의 라벨, 텍스트, 오프셋 값 저장 변수 \n",
    "    label_list, text_list= [], []\n",
    "    \n",
    "    # 1개씩 뉴스기사, 라벨 추출 해서 저장 \n",
    "    for (_label, _text) in batch:\n",
    "         # 라벨 인코딩 후 저장\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         \n",
    "         # 텍스트 인코딩 후 저장\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         \n",
    "    \n",
    "    # 텐서화 진행     \n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    \n",
    "    # 문장의 길이 일치\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 학습용, 검증용, 테스트용 DataSet 준비 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "### 학습용, 검증용, 테스트용 Dataset, DataLoader 준비\n",
    "num_train = int(len(trainDS) * 0.95)\n",
    "print(f' num_train :{num_train}')\n",
    "\n",
    "split_trainDS, split_validDS= random_split( trainDS, [num_train, len(trainDS) - num_train])\n",
    "print(f' len(split_trainDS) :{len(split_trainDS)}')\n",
    "print(f' len(split_validDS) :{len(split_validDS)}')\n",
    "\n",
    "trainDL = DataLoader( split_trainDS, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch )\n",
    "validDL = DataLoader( split_validDS, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch )\n",
    "testDL  = DataLoader( testDS,        batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' len(trainDL) :{len(trainDL)*BATCH_SIZE}')\n",
    "print(f' len(validDL) :{len(validDL)*BATCH_SIZE}')\n",
    "print(f' len(testDL) :{len(testDL)* BATCH_SIZE}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
