{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [한글 데이터셋 RNN] <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-25\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-25\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"nsmc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    50000 non-null  object\n",
      " 1   label   50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "nsmcDF = pd.DataFrame(corpus.test)\n",
    "nsmcDF.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45000 entries, 33553 to 6838\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    45000 non-null  object\n",
      " 1   label   45000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "trainDF = nsmcDF.sample(frac=0.9, random_state=42)\n",
    "trainDF.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5000 entries, 9 to 49997\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.2+ KB\n"
     ]
    }
   ],
   "source": [
    "testDF = nsmcDF.drop(trainDF.index)\n",
    "testDF.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [2] 단어 사전 생성 <hr>\n",
    "    -   토큰화 진행 ==> 형태소 분석기 선택\n",
    "    -   단어 사전\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [2-1] 토큰화 진행 ==> 문장 --> 단어\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===> 모듈 로딩\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "### 토큰화 인스턴스 생성\n",
    "tokenizer = Okt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 문장 ====> [단어 분리]\n",
    "# for text in trainDF.text:\n",
    "#     print(tokenizer.morphs(text, stem=True))\n",
    "#     break\n",
    "train_tokens = [tokenizer.morphs(text, stem=True) for text in trainDF.text]\n",
    "test_tokens = [tokenizer.morphs(text, stem=True) for text in testDF.text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_tokens] 45000개\n",
      "[test_tokens] 5000개\n",
      "[train_tokens[0]] 19개\n",
      "[test_tokens[0]] 18개\n",
      "[train_tokens[1]] 14개\n",
      "[test_tokens[1]] 6개\n"
     ]
    }
   ],
   "source": [
    "print(f\"[train_tokens] {len(train_tokens)}개\\n[test_tokens] {len(test_tokens)}개\")\n",
    "print(\n",
    "    f\"[train_tokens[0]] {len(train_tokens[0])}개\\n[test_tokens[0]] {len(test_tokens[0])}개\"\n",
    ")\n",
    "print(\n",
    "    f\"[train_tokens[1]] {len(train_tokens[1])}개\\n[test_tokens[1]] {len(test_tokens[1])}개\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [2-2] 토큰 ===> 단어/어휘 사전 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어 사전 생성 함수\n",
    "def build_vocab(corpus, vocab_size, special_token):\n",
    "    counter = Counter()\n",
    "\n",
    "    # 단어/토큰에 대한 빈도수 계산\n",
    "    for token in corpus:\n",
    "        for pun in string.punctuation:\n",
    "            if pun in token:\n",
    "                token.remove(pun)\n",
    "        counter.update(token)\n",
    "\n",
    "    # 단어/어휘 사전 생성\n",
    "    vocab = special_token\n",
    "\n",
    "    # 단어/어휘 사전에 빈도수가 높은 단어 추가\n",
    "    for token, count in counter.most_common(vocab_size):\n",
    "        vocab.append(token)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = build_vocab(train_tokens, 5000, [\"<PAD>\", \"<UNK>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB] ---> 5002개\n",
      "['<PAD>', '<UNK>', '이', '영화', '보다', '하다', '의', '..', '에', '가', '.', '...', '을', '도', '들', '는', '를', '은', '없다', '이다', '있다', '좋다', '너무', '다', '정말', '한', '되다', '재밌다', '만', '진짜']\n"
     ]
    }
   ],
   "source": [
    "print(f\"[VOCAB] ---> {len(VOCAB)}개\\n{VOCAB[:30]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [2-3] 인코딩 & 디코딩 인덱싱\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인코딩 : 문자 >>>> 숫자로 변환\n",
    "token_to_id = {vo: id for id, vo in enumerate(VOCAB)}\n",
    "\n",
    "### 디코딩 : 숫자 >>>> 문자로 변환\n",
    "id_to_token = {id: vo for id, vo in enumerate(VOCAB)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [3] 데이터 가공 <hr>\n",
    "    -   토큰 데이터 정수 인코딩\n",
    "    -   데이터 길이 표준화 => 다른 길이의 데이터를 길이 맞추기 -> 1개 문장 구성하는 단어 수 맞추기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [3-1] 토큰 정수화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 리뷰의 문자를 정수로 변환 및 단어/어휘 사전에 없는 문자도 처리\n",
    "unk_id = token_to_id[\"<UNK>\"]\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in text] for text in train_tokens\n",
    "]\n",
    "test_ids = [[token_to_id.get(token, unk_id) for token in text] for text in test_tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [3-2] 데이터 구성 단어 수 맞추기 즉, 패딩(padding)\n",
    "    -   단어 수 선정 필요\n",
    "    -   선정된 단어 수에 맞게 데이터 조절 => 길면 잘라내기, 짧으면 채우기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2], [9, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\n",
    "b = 2\n",
    "a[:2], a[-b:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 패딩 처리 함수\n",
    "# - sentences : 토큰화된 문장 데이터\n",
    "# - max_length : 최대 문장 길이 즉, 1개 문장 구성 단어 수\n",
    "# - pad : 패딩 처리 시 추가될 문자 값\n",
    "# - start : 패딩 시 처리 방향 [기:R 오른쪽 즉, 뒷부분 자르기/추가하기]\n",
    "def pad_sequence(sentences, max_length, pad, start=\"R\"):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == \"R\" else sen[-max_length:]\n",
    "        padd_sen = (\n",
    "            (sen + [pad] * (max_length - len(sen)))\n",
    "            if start == \"R\"\n",
    "            else ([pad] * (max_length - len(sen)) + sen)\n",
    "        )\n",
    "        result.append(padd_sen)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = token_to_id[\"<PAD>\"]\n",
    "MAX_LENGTH = 32\n",
    "train_ids = pad_sequence(train_ids, MAX_LENGTH, PAD_ID)\n",
    "test_ids = pad_sequence(test_ids, MAX_LENGTH, PAD_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_ids[0]] ---> 32개\n",
      "[256, 1610, 12, 1362, 167, 219, 361, 3, 2090, 1038, 253, 33, 3985, 1, 1, 1023, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[test_ids[0]] ---> 32개\n"
     ]
    }
   ],
   "source": [
    "print(f\"[train_ids[0]] ---> {len(train_ids[0])}개\\n{train_ids[0]}\")\n",
    "print(f\"[test_ids[0]] ---> {len(test_ids[0])}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [4] 학습 준비 <hr>\n",
    "    -   데이터 로더 준비\n",
    "    -   학습용/테스트용 함수\n",
    "    -   모델 클래스\n",
    "    -   학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH_SIZE, LOSS_FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS => torch.Size([45000, 32]), labelTS => torch.Size([45000])\n"
     ]
    }
   ],
   "source": [
    "### ==> 데이터셋 생성 : List >>>> Tensor\n",
    "# 학습용 데이터셋\n",
    "dataTS = torch.LongTensor(train_ids)\n",
    "labelTS = torch.FloatTensor(trainDF.label.values)\n",
    "\n",
    "print(f\"dataTS => {dataTS.shape}, labelTS => {labelTS.shape}\")\n",
    "\n",
    "trainDS = TensorDataset(dataTS, labelTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataTS_ => torch.Size([5000, 32]), labelTS_ => torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터셋\n",
    "dataTS_ = torch.LongTensor(test_ids)\n",
    "labelTS_ = torch.FloatTensor(testDF.label.values)\n",
    "\n",
    "print(f\"dataTS_ => {dataTS_.shape}, labelTS_ => {labelTS_.shape}\")\n",
    "\n",
    "testDS = TensorDataset(dataTS_, labelTS_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ==> 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDL = DataLoader(trainDS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testDL = DataLoader(testDS, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [4-2] 모델 클래스 정의\n",
    "    -   입력층 : Embedding Layer\n",
    "    -   은닉층 : RNN/LSTM Layer\n",
    "    -   은닉층 : Dropout Layer\n",
    "    -   출력층 : Linear Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layer,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab, embedding_dim=embedding_dim, padding_idx=0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layer,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layer,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layer = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab=n_vocab, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layer=n_layer\n",
    ").to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = optim.RMSprop(classifier.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.6981192827224731\n",
      "Train Loss 500 : 0.6728089472014985\n",
      "Train Loss 1000 : 0.6483240639651334\n",
      "Val Loss : 0.6151549288421679, Val Accuracy : 0.6624\n",
      "Train Loss 0 : 0.5543519854545593\n",
      "Train Loss 500 : 0.5826633009249103\n",
      "Train Loss 1000 : 0.5664007042671417\n",
      "Val Loss : 0.503889705724777, Val Accuracy : 0.7748\n",
      "Train Loss 0 : 0.6051123142242432\n",
      "Train Loss 500 : 0.447650252612765\n",
      "Train Loss 1000 : 0.4324425974568644\n",
      "Val Loss : 0.4439732933500011, Val Accuracy : 0.7936\n",
      "Train Loss 0 : 0.38300520181655884\n",
      "Train Loss 500 : 0.3621136610379476\n",
      "Train Loss 1000 : 0.35731632111372646\n",
      "Val Loss : 0.3828697977172341, Val Accuracy : 0.8246\n",
      "Train Loss 0 : 0.24730445444583893\n",
      "Train Loss 500 : 0.31350751969212304\n",
      "Train Loss 1000 : 0.3182889421160643\n",
      "Val Loss : 0.3992704557385414, Val Accuracy : 0.8336\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (input_ids, labels) in enumerate(datasets):\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "            yhat = torch.sigmoid(logits) > 0.5\n",
    "            corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, trainDL, criterion, optimizer, device, interval)\n",
    "    test(classifier, testDL, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
